{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsdatacsv = pd.read_csv(\"../../resources/20180622processedGoldStandardTopics.tsv.gz\", delimiter=\"\\t\")\n",
    "preddatacsv = pd.read_csv(\"../../resources/20180622processedGoldStandardTopics.tsv.gz\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the data frames to be multi-indexed with a topic index for the topic number where the documents ids\n",
    "# are grouped into\n",
    "gsdata = gsdatacsv.set_index(['trec_topic_number', 'trec_doc_id']).sort_index()\n",
    "preddata = preddatacsv.set_index(['trec_topic_number', 'trec_doc_id']).sort_index()\n",
    "if gsdata.shape[0] != preddata.shape[0]:\n",
    "    raise ValueError(\"The gold standard has \" + gsdata.shape[0] + \" rows but the predicted data has \" + preddata.shape[0])\n",
    "joined = gsdata.join(preddata, lsuffix=\"_gs\", rsuffix=\"_pred\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 246 246 738\n"
     ]
    }
   ],
   "source": [
    "g1 = gsdata.loc[30][\"gene1_annotation_desc\"].dropna().shape[0]\n",
    "g2 = gsdata.loc[30][\"gene2_annotation_desc\"].dropna().shape[0]\n",
    "g3 = gsdata.loc[30][\"gene3_annotation_desc\"].dropna().shape[0]\n",
    "print(g1, g2, g3, g1+g2+g3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalColumns(cols, printTopics=True):\n",
    "    # This builds the list of columns in the joined dataframe be suffixing all the given column\n",
    "    # names with the _gs and _pred suffixes\n",
    "    collist = list(chain.from_iterable((col+\"_gs\", col+\"_pred\") for col in cols))\n",
    "    \n",
    "    # Extract exactly those columns we want to compare\n",
    "    cols_joined = joined[collist]\n",
    "\n",
    "    # The 'confusion labels'. To have the confusion matrix to be comparable over the topics,\n",
    "    # we need to always give the same list of labels\n",
    "    conf_labels = set([])\n",
    "    for col in collist:\n",
    "        conf_labels.update(joined[col].dropna().values)\n",
    "    conf_labels = sorted(list(conf_labels))\n",
    "    # We will collect all the labels of all topic here to get overall results\n",
    "    allgslabels   = None\n",
    "    allpredlabels = None\n",
    "    # Iterate over the topics (first part of the multi index)\n",
    "    for i in cols_joined.index.levels[0]:\n",
    "        topici     = cols_joined.loc[i]\n",
    "        gslabels   = None\n",
    "        predlabels = None\n",
    "        # We will now concatenate the lists of labels from the different columns that are given.\n",
    "        # The idea is that we have multiple genes for some topics which have their own columns,\n",
    "        # geneX_annotation_desc. We are interested into how well we can recognize the gene label,\n",
    "        # so we want all the information in a single column as input into the scoring functions below\n",
    "        for col in list(filter(lambda s: s.endswith(\"_gs\"), collist)):\n",
    "            gslabels = pd.concat([gslabels, topici[col]])\n",
    "        for col in list(filter(lambda s: s.endswith(\"_pred\"), collist)):\n",
    "            predlabels = pd.concat([predlabels, topici[col]])\n",
    "        \n",
    "        joinedlabels = pd.DataFrame({\"gslabels\": gslabels, \"predlabels\": predlabels})\n",
    "\n",
    "        if (joinedlabels[\"gslabels\"].notnull() & joinedlabels[\"predlabels\"].isnull()).any():\n",
    "            raise ValueError(\"The prediction contains null values where the gold standard has a value\")\n",
    "        \n",
    "        joinedlabels = joinedlabels.dropna()\n",
    "        gslabels   = joinedlabels[\"gslabels\"]\n",
    "        predlabels = joinedlabels[\"predlabels\"]\n",
    "        \n",
    "        allgslabels = pd.concat([allgslabels, gslabels])\n",
    "        allpredlabels = pd.concat([allpredlabels, predlabels])\n",
    "        \n",
    "        if printTopics:\n",
    "            print(\"Topic\", i)\n",
    "            print(confusion_matrix(gslabels, predlabels, conf_labels))\n",
    "            print(conf_labels)\n",
    "            print(\"ACC:\", accuracy_score(gslabels, predlabels))\n",
    "       \n",
    "    print(\"Allover\")\n",
    "    print(confusion_matrix(allgslabels, allpredlabels, conf_labels))\n",
    "    print(conf_labels)\n",
    "    print(\"ACC:\", accuracy_score(allgslabels, allpredlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allover\n",
      "[[4149    0    0    0]\n",
      " [   0  938    0    0]\n",
      " [   0    0 1273    0]\n",
      " [   0    0    0 2914]]\n",
      "['Exact', 'More General', 'More Specific', 'Not Disease']\n",
      "ACC: 1.0\n"
     ]
    }
   ],
   "source": [
    "evalColumns([\"disease_desc\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allover\n",
      "[[ 651    0    0    0]\n",
      " [   0 5065    0    0]\n",
      " [   0    0 3974    0]\n",
      " [   0    0    0 1655]]\n",
      "['Different Variant', 'Exact', 'Missing Gene', 'Missing Variant']\n",
      "ACC: 1.0\n"
     ]
    }
   ],
   "source": [
    "evalColumns([\"gene1_annotation_desc\", \"gene2_annotation_desc\", \"gene3_annotation_desc\"], False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
