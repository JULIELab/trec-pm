{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import json\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING pyltr with LETOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"/Users/ari/Downloads/MQ2007/Fold1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(folder,'train.txt')) as trainfile, open(join(folder,'vali.txt')) as valifile, open(join(folder,'test.txt')) as evalfile:\n",
    "    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pyltr.metrics.NDCG(k=10)\n",
    "\n",
    "# Only needed if you want to perform validation (early stopping & trimming)\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(VX, Vy, Vqids, metric=metric, stop_after=250)\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.02,\n",
    "    max_features=0.5,\n",
    "    query_subsample=0.5,\n",
    "    max_leaf_nodes=10,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.fit(TX, Ty, Tqids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epred = model.predict(EX)\n",
    "print('Random ranking:', metric.calc_mean_random(Eqids, Ey))\n",
    "print('Our model:', metric.calc_mean(Eqids, Ey, Epred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading GS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsPath = \"/Users/ari/Downloads/TREC/trec2018/results/goldstandard\"\n",
    "trainYear = \"2017\"\n",
    "testYear = \"2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsTrainFile = join(join(gsPath, trainYear),\"20180622processedGoldStandardXMLTXT.tsv\")\n",
    "trecEvalTrain = \"/Users/ari/Downloads/TREC/trec-pm/resources/topics2017.xml\"\n",
    "\n",
    "gsTestFile = join(join(gsPath, testYear),\"20190111processedGoldStandardPub2018.tsv\")\n",
    "trecEvalTest = \"/Users/ari/Downloads/TREC/trec-pm/resources/topics2018.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trec_topic_number</th>\n",
       "      <th>trec_doc_id</th>\n",
       "      <th>pm_rel_desc</th>\n",
       "      <th>disease_desc</th>\n",
       "      <th>gene1_annotation_desc</th>\n",
       "      <th>gene1_name</th>\n",
       "      <th>gene2_annotation_desc</th>\n",
       "      <th>gene2_name</th>\n",
       "      <th>gene3_annotation_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>abstract</th>\n",
       "      <th>major_mesh</th>\n",
       "      <th>minor_mesh</th>\n",
       "      <th>trec_topic_disease</th>\n",
       "      <th>trec_topic_age</th>\n",
       "      <th>trec_topic_sex</th>\n",
       "      <th>trec_topic_other1</th>\n",
       "      <th>trec_topic_other2</th>\n",
       "      <th>trec_topic_other3</th>\n",
       "      <th>trec_topic_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10065107</td>\n",
       "      <td>Human PM</td>\n",
       "      <td>Exact</td>\n",
       "      <td>Missing Gene</td>\n",
       "      <td>CDK4 Amplification</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>We reported a 36-year-old woman with metastati...</td>\n",
       "      <td></td>\n",
       "      <td>Adult;Antineoplastic Combined Chemotherapy Pro...</td>\n",
       "      <td>Liposarcoma</td>\n",
       "      <td>38-year-old</td>\n",
       "      <td>male</td>\n",
       "      <td>GERD</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CDK4 Amplification</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  trec_topic_number trec_doc_id pm_rel_desc disease_desc  \\\n",
       "0           0                  1    10065107    Human PM        Exact   \n",
       "\n",
       "  gene1_annotation_desc          gene1_name gene2_annotation_desc gene2_name  \\\n",
       "0          Missing Gene  CDK4 Amplification                                    \n",
       "\n",
       "  gene3_annotation_desc         ...          \\\n",
       "0                               ...           \n",
       "\n",
       "                                            abstract major_mesh  \\\n",
       "0  We reported a 36-year-old woman with metastati...              \n",
       "\n",
       "                                          minor_mesh  trec_topic_disease  \\\n",
       "0  Adult;Antineoplastic Combined Chemotherapy Pro...         Liposarcoma   \n",
       "\n",
       "  trec_topic_age trec_topic_sex trec_topic_other1 trec_topic_other2  \\\n",
       "0    38-year-old           male              GERD                     \n",
       "\n",
       "  trec_topic_other3     trec_topic_gene  \n",
       "0                    CDK4 Amplification  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading 2017 Topics\n",
    "topicsColumns = ['trec_topic_number', 'trec_topic_gene']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(trecEvalTrain)\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    gene = topic.find('gene').text\n",
    "    topics = topics.append(pd.Series([topicNumber, gene], index=topicsColumns), ignore_index=True)\n",
    "topics['trec_topic_number'] = topics['trec_topic_number'].astype('int')\n",
    "\n",
    "# Merging\n",
    "train = pd.read_csv(gsTrainFile, sep = '\\t', encoding='utf8')\n",
    "train.fillna(\"\", inplace=True)\n",
    "trainData = train.merge(topics, left_on=['trec_topic_number'], right_on=['trec_topic_number'], how='left')\n",
    "trainData.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trec_topic_number</th>\n",
       "      <th>trec_doc_id</th>\n",
       "      <th>pm_rel_desc</th>\n",
       "      <th>disease_desc</th>\n",
       "      <th>gene1_annotation_desc</th>\n",
       "      <th>gene1_name</th>\n",
       "      <th>gene2_annotation_desc</th>\n",
       "      <th>gene2_name</th>\n",
       "      <th>gene3_annotation_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>other_desc</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>major_mesh</th>\n",
       "      <th>minor_mesh</th>\n",
       "      <th>trec_topic_disease</th>\n",
       "      <th>trec_topic_age</th>\n",
       "      <th>trec_topic_sex</th>\n",
       "      <th>trec_topic_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1007359</td>\n",
       "      <td>Human PM</td>\n",
       "      <td>More Specific</td>\n",
       "      <td>Missing Gene</td>\n",
       "      <td>BRAF (V600E)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Not Discussed</td>\n",
       "      <td>0</td>\n",
       "      <td>[Primary multiple malignant melanomas of unusu...</td>\n",
       "      <td>In 1975, 117 patients with malignant melanoma ...</td>\n",
       "      <td>Melanoma/pathology;Neoplasms, Multiple Primary...</td>\n",
       "      <td>Adult;Aged;Female;Humans;Male;Middle Aged;Skin...</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>64-year-old</td>\n",
       "      <td>male</td>\n",
       "      <td>BRAF (V600E)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  trec_topic_number trec_doc_id pm_rel_desc   disease_desc  \\\n",
       "0           0                  1     1007359    Human PM  More Specific   \n",
       "\n",
       "  gene1_annotation_desc    gene1_name gene2_annotation_desc gene2_name  \\\n",
       "0          Missing Gene  BRAF (V600E)                                    \n",
       "\n",
       "  gene3_annotation_desc       ...           other_desc relevance_score  \\\n",
       "0                             ...        Not Discussed               0   \n",
       "\n",
       "                                               title  \\\n",
       "0  [Primary multiple malignant melanomas of unusu...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In 1975, 117 patients with malignant melanoma ...   \n",
       "\n",
       "                                          major_mesh  \\\n",
       "0  Melanoma/pathology;Neoplasms, Multiple Primary...   \n",
       "\n",
       "                                          minor_mesh trec_topic_disease  \\\n",
       "0  Adult;Aged;Female;Humans;Male;Middle Aged;Skin...           melanoma   \n",
       "\n",
       "  trec_topic_age trec_topic_sex trec_topic_gene  \n",
       "0    64-year-old           male    BRAF (V600E)  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading 2018 Topics\n",
    "topicsColumns = ['trec_topic_number', 'trec_topic_gene']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(trecEvalTest)\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    gene = topic.find('gene').text\n",
    "    topics = topics.append(pd.Series([topicNumber, gene], index=topicsColumns), ignore_index=True)\n",
    "topics['trec_topic_number'] = topics['trec_topic_number'].astype('int')\n",
    "\n",
    "# Merging\n",
    "testVal = pd.read_csv(gsTestFile, sep = '\\t', encoding='utf8', dtype={'trec_doc_id':object})\n",
    "testVal.fillna(\"\", inplace=True)\n",
    "testValData = testVal.merge(topics, left_on=['trec_topic_number'], right_on=['trec_topic_number'], how='left')\n",
    "testValData.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to tokenize, remove stop words, get stemms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get Stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "def tokenizePorter(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for item in tokens:\n",
    "        if item not in stopWords: \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return ' '.join(stems)\n",
    "\n",
    "def tokenizeSnowball(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for item in tokens:\n",
    "        if item not in stopWords: \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>qid</th>\n",
       "      <th>title_stemmed</th>\n",
       "      <th>abstract_stemmed</th>\n",
       "      <th>mesh_stemmed</th>\n",
       "      <th>title_abstract_mesh_stemmed</th>\n",
       "      <th>disease_stemmed</th>\n",
       "      <th>gene_stemmed</th>\n",
       "      <th>trec_doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12017</td>\n",
       "      <td>case metastat liposarcoma origin retroperitoneum success treat combin chemotherapi</td>\n",
       "      <td>report 36yearold woman metastat liposarcoma origin retroperitoneum respond well adjuv chemotherapi primari tumor remov surgeri two month later patient develop metastasi brain lung four month later metastat liposarcoma brain general extrem rare patient treat combin chemotherapi use cyclophosphamid vincristin adriamycin dacarbazin cyvad examin former two drug altern vindesin ifosfamid anoth regimen cisplatin etoposid given threeweek interv result metastas total disappear recurr lesion note two year although role chemotherapi liposarcoma well defin littl data support use adjuv set combin chemotherapi seem effect advanc liposarcoma</td>\n",
       "      <td>adult antineoplast combin chemotherapi protocol therapeut use brain neoplasm drug therapi brain neoplasm secondari cyclophosphamid administr dosag dacarbazin administr dosag doxorubicin administr dosag drug administr schedul femal human liposarcoma drug therapi liposarcoma secondari lung neoplasm drug therapi lung neoplasm secondari remiss induct retroperiton neoplasm patholog vincristin administr dosag</td>\n",
       "      <td>case metastat liposarcoma origin retroperitoneum success treat combin chemotherapi report 36yearold woman metastat liposarcoma origin retroperitoneum respond well adjuv chemotherapi primari tumor remov surgeri two month later patient develop metastasi brain lung four month later metastat liposarcoma brain general extrem rare patient treat combin chemotherapi use cyclophosphamid vincristin adriamycin dacarbazin cyvad examin former two drug altern vindesin ifosfamid anoth regimen cisplatin etoposid given threeweek interv result metastas total disappear recurr lesion note two year although role chemotherapi liposarcoma well defin littl data support use adjuv set combin chemotherapi seem effect advanc liposarcoma adult antineoplast combin chemotherapi protocol therapeut use brain neoplasm drug therapi brain neoplasm secondari cyclophosphamid administr dosag dacarbazin administr dosag doxorubicin administr dosag drug administr schedul femal human liposarcoma drug therapi liposarcoma secondari lung neoplasm drug therapi lung neoplasm secondari remiss induct retroperiton neoplasm patholog vincristin administr dosag</td>\n",
       "      <td>liposarcoma</td>\n",
       "      <td>cdk4 amplif</td>\n",
       "      <td>10065107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance_score    qid  \\\n",
       "0  0                12017   \n",
       "\n",
       "                                                                        title_stemmed  \\\n",
       "0  case metastat liposarcoma origin retroperitoneum success treat combin chemotherapi   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstract_stemmed  \\\n",
       "0  report 36yearold woman metastat liposarcoma origin retroperitoneum respond well adjuv chemotherapi primari tumor remov surgeri two month later patient develop metastasi brain lung four month later metastat liposarcoma brain general extrem rare patient treat combin chemotherapi use cyclophosphamid vincristin adriamycin dacarbazin cyvad examin former two drug altern vindesin ifosfamid anoth regimen cisplatin etoposid given threeweek interv result metastas total disappear recurr lesion note two year although role chemotherapi liposarcoma well defin littl data support use adjuv set combin chemotherapi seem effect advanc liposarcoma   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                             mesh_stemmed  \\\n",
       "0  adult antineoplast combin chemotherapi protocol therapeut use brain neoplasm drug therapi brain neoplasm secondari cyclophosphamid administr dosag dacarbazin administr dosag doxorubicin administr dosag drug administr schedul femal human liposarcoma drug therapi liposarcoma secondari lung neoplasm drug therapi lung neoplasm secondari remiss induct retroperiton neoplasm patholog vincristin administr dosag   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             title_abstract_mesh_stemmed  \\\n",
       "0  case metastat liposarcoma origin retroperitoneum success treat combin chemotherapi report 36yearold woman metastat liposarcoma origin retroperitoneum respond well adjuv chemotherapi primari tumor remov surgeri two month later patient develop metastasi brain lung four month later metastat liposarcoma brain general extrem rare patient treat combin chemotherapi use cyclophosphamid vincristin adriamycin dacarbazin cyvad examin former two drug altern vindesin ifosfamid anoth regimen cisplatin etoposid given threeweek interv result metastas total disappear recurr lesion note two year although role chemotherapi liposarcoma well defin littl data support use adjuv set combin chemotherapi seem effect advanc liposarcoma adult antineoplast combin chemotherapi protocol therapeut use brain neoplasm drug therapi brain neoplasm secondari cyclophosphamid administr dosag dacarbazin administr dosag doxorubicin administr dosag drug administr schedul femal human liposarcoma drug therapi liposarcoma secondari lung neoplasm drug therapi lung neoplasm secondari remiss induct retroperiton neoplasm patholog vincristin administr dosag   \n",
       "\n",
       "  disease_stemmed gene_stemmed trec_doc_id  \n",
       "0  liposarcoma     cdk4 amplif  10065107    "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Preprocessing the Text\n",
    "removePunctuation = str.maketrans('\\n', ' ', string.punctuation)\n",
    "\n",
    "# Transforms the text to lower case, remove punctuations, get the stemms of words \n",
    "trainData['title_abstract_mesh'] = trainData[['title', 'abstract', \"major_mesh\", \"minor_mesh\"]].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1)\n",
    "trainData['title_abstract_mesh_stemmed'] = trainData['title_abstract_mesh'].apply(tokenizeSnowball)\n",
    "trainData['title_stemmed'] = trainData[['title']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['abstract_stemmed'] = trainData[['abstract']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['mesh_stemmed'] = trainData[['major_mesh', 'minor_mesh']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['disease_stemmed'] = trainData[['trec_topic_disease']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['gene_stemmed'] = trainData[['trec_topic_gene']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "\n",
    "# Defining query ids\n",
    "trainData[\"qid\"] = trainData[\"trec_topic_number\"].astype(str)+str(trainYear)\n",
    "trainDataSliced = trainData[['relevance_score','qid', 'title_stemmed', 'abstract_stemmed', 'mesh_stemmed', 'title_abstract_mesh_stemmed', 'disease_stemmed', 'gene_stemmed', 'trec_doc_id']]\n",
    "trainDataSliced.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>qid</th>\n",
       "      <th>title_stemmed</th>\n",
       "      <th>abstract_stemmed</th>\n",
       "      <th>mesh_stemmed</th>\n",
       "      <th>title_abstract_mesh_stemmed</th>\n",
       "      <th>disease_stemmed</th>\n",
       "      <th>gene_stemmed</th>\n",
       "      <th>trec_doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12018</td>\n",
       "      <td>primari multipl malign melanoma unusu long durat</td>\n",
       "      <td>1975 117 patient malign melanoma attent depart dermatolog univers cologn three superfici spread melanoma ssm long periodon patient probabl 25 year patient anoth primari malign melanoma develop though metastasi appear ssm give protect melanoma superfici nodular type comparison 70yearold patient pigment tumour 40 year sudden spread five year ago howev least metastas regress form vitiligolik lesion surround deposit melanin macrophag case spontan regress protect patient new metastas lymph node probabl liver well</td>\n",
       "      <td>melanoma patholog neoplasm multipl primari patholog skin neoplasm patholog adult age femal human male middl age skin patholog time factor</td>\n",
       "      <td>primari multipl malign melanoma unusu long durat 1975 117 patient malign melanoma attent depart dermatolog univers cologn three superfici spread melanoma ssm long periodon patient probabl 25 year patient anoth primari malign melanoma develop though metastasi appear ssm give protect melanoma superfici nodular type comparison 70yearold patient pigment tumour 40 year sudden spread five year ago howev least metastas regress form vitiligolik lesion surround deposit melanin macrophag case spontan regress protect patient new metastas lymph node probabl liver well melanoma patholog neoplasm multipl primari patholog skin neoplasm patholog adult age femal human male middl age skin patholog time factor</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>braf v600e</td>\n",
       "      <td>1007359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance_score    qid                                     title_stemmed  \\\n",
       "0  0                12018  primari multipl malign melanoma unusu long durat   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    abstract_stemmed  \\\n",
       "0  1975 117 patient malign melanoma attent depart dermatolog univers cologn three superfici spread melanoma ssm long periodon patient probabl 25 year patient anoth primari malign melanoma develop though metastasi appear ssm give protect melanoma superfici nodular type comparison 70yearold patient pigment tumour 40 year sudden spread five year ago howev least metastas regress form vitiligolik lesion surround deposit melanin macrophag case spontan regress protect patient new metastas lymph node probabl liver well   \n",
       "\n",
       "                                                                                                                                mesh_stemmed  \\\n",
       "0  melanoma patholog neoplasm multipl primari patholog skin neoplasm patholog adult age femal human male middl age skin patholog time factor   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    title_abstract_mesh_stemmed  \\\n",
       "0  primari multipl malign melanoma unusu long durat 1975 117 patient malign melanoma attent depart dermatolog univers cologn three superfici spread melanoma ssm long periodon patient probabl 25 year patient anoth primari malign melanoma develop though metastasi appear ssm give protect melanoma superfici nodular type comparison 70yearold patient pigment tumour 40 year sudden spread five year ago howev least metastas regress form vitiligolik lesion surround deposit melanin macrophag case spontan regress protect patient new metastas lymph node probabl liver well melanoma patholog neoplasm multipl primari patholog skin neoplasm patholog adult age femal human male middl age skin patholog time factor   \n",
       "\n",
       "  disease_stemmed gene_stemmed trec_doc_id  \n",
       "0  melanoma        braf v600e   1007359     "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataSetSliced = []\n",
    "valDataSetSliced = []\n",
    "\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Preprocessing the Text\n",
    "removePunctuation = str.maketrans('\\n', ' ', string.punctuation)\n",
    "\n",
    "# Transforms the text to lower case, remove punctuations, get the stemms of words \n",
    "testValData['title_abstract_mesh'] = testValData[['title', 'abstract', \"major_mesh\", \"minor_mesh\"]].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1)\n",
    "testValData['title_abstract_mesh_stemmed'] = testValData['title_abstract_mesh'].apply(tokenizeSnowball)\n",
    "testValData['title_stemmed'] = testValData[['title']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['abstract_stemmed'] = testValData[['abstract']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['mesh_stemmed'] = testValData[['major_mesh', 'minor_mesh']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['disease_stemmed'] = testValData[['trec_topic_disease']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['gene_stemmed'] = testValData[['trec_topic_gene']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "\n",
    "# Defining query ids\n",
    "testValData[\"qid\"] = testValData[\"trec_topic_number\"].astype(str)+str(testYear)\n",
    "testValDataSliced = testValData[['relevance_score','qid', 'title_stemmed', 'abstract_stemmed', 'mesh_stemmed', 'title_abstract_mesh_stemmed', 'disease_stemmed', 'gene_stemmed', 'trec_doc_id']]\n",
    "testValDataSliced.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for Disease and Gene in Title, Abstract, Mesh and Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countTerms(terms, target):\n",
    "    vectorizer = CountVectorizer(vocabulary = terms)\n",
    "    transformed_data = vectorizer.fit_transform(target)\n",
    "\n",
    "    score = pd.DataFrame(transformed_data.toarray(), columns=vectorizer.get_feature_names())\n",
    "    scoreDict = score.to_dict('records')\n",
    "    return scoreDict\n",
    "    \n",
    "def specificTermsCount(allTermsCount, index, terms):\n",
    "    termsCount = 0\n",
    "    termList = terms.split()\n",
    "    for term in termList:\n",
    "        termsCount += allTermsCount[index][term]\n",
    "    return(termsCount)\n",
    "\n",
    "def percentageOfTermsMatched(allTermsCount, index, terms):\n",
    "    matched = 0\n",
    "    termList = terms.split()\n",
    "    for term in termList:\n",
    "        if allTermsCount[index][term] > 0:\n",
    "            matched += 1\n",
    "    return(matched/len(termList))\n",
    "\n",
    "def termsFrequency(corpus, termCount):\n",
    "    count = len(corpus.split())\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return float(termCount/count)\n",
    "\n",
    "def tfidfWeights(terms, target):\n",
    "    tvec = TfidfVectorizer(vocabulary = terms)\n",
    "    weights = tvec.fit_transform(target)\n",
    "    \n",
    "    score = pd.DataFrame(weights.toarray(), columns=tvec.get_feature_names())\n",
    "    scoreDict = score.to_dict('records')\n",
    "    return scoreDict\n",
    "\n",
    "def specificTermsTfIdf(tfidfWeights, index, terms):\n",
    "    tfidf = 0\n",
    "    termList = terms.split()\n",
    "    for term in termList:\n",
    "        tfidf += tfidfWeights[index][term]\n",
    "    return(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures (diseaseTerms, geneTerms, trainDataSliced):\n",
    "    # TITLE\n",
    "    \n",
    "    # Disease\n",
    "    termsCountTitle = countTerms(diseaseTerms, trainDataSliced['title_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_title_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountTitle, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_title_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_stemmed'], row['disease_title_count']), axis=1)\n",
    "    trainDataSliced['disease_title_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountTitle, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfTitle = tfidfWeights(diseaseTerms, trainDataSliced['title_stemmed'])\n",
    "    trainDataSliced['disease_title_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfTitle, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "\n",
    "    # Gene\n",
    "    geneCountTitle = countTerms(geneTerms, trainDataSliced['title_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_title_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountTitle, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_title_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_stemmed'], row['gene_title_count']), axis=1)\n",
    "    trainDataSliced['gene_title_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountTitle, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfTitle = tfidfWeights(geneTerms, trainDataSliced['title_stemmed'])\n",
    "    trainDataSliced['gene_title_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfTitle, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease and Gene\n",
    "    trainDataSliced['disease_gene_title_tf'] = trainDataSliced['disease_title_tf'] + trainDataSliced['gene_title_tf']\n",
    "    trainDataSliced['disease_gene_title_tfidf'] = trainDataSliced['disease_title_tfidf'] + trainDataSliced['gene_title_tfidf']\n",
    "\n",
    "    # ABSTRACT\n",
    "\n",
    "    # Disease\n",
    "    termsCountAbst = countTerms(diseaseTerms, trainDataSliced['abstract_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_abstract_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountAbst, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_abstract_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['abstract_stemmed'], row['disease_abstract_count']), axis=1)\n",
    "    trainDataSliced['disease_abstract_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountAbst, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfAbstract = tfidfWeights(diseaseTerms, trainDataSliced['abstract_stemmed'])\n",
    "    trainDataSliced['disease_abstract_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfAbstract, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    # Gene\n",
    "    geneCountAbst = countTerms(geneTerms, trainDataSliced['abstract_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_abstract_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountAbst, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_abstract_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['abstract_stemmed'], row['gene_abstract_count']), axis=1)\n",
    "    trainDataSliced['gene_abstract_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountAbst, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfAbstract = tfidfWeights(geneTerms, trainDataSliced['abstract_stemmed'])\n",
    "    trainDataSliced['gene_abstract_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfAbstract, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease + Gene\n",
    "    trainDataSliced['disease_gene_abstract_tf'] = trainDataSliced['disease_abstract_tf'] + trainDataSliced['gene_abstract_tf']\n",
    "    trainDataSliced['disease_gene_abstract_tfidf'] = trainDataSliced['disease_abstract_tfidf'] + trainDataSliced['gene_abstract_tfidf']\n",
    "\n",
    "    # MESH\n",
    "\n",
    "    # Disease\n",
    "    termsCountMesh = countTerms(diseaseTerms, trainDataSliced['mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_mesh_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountMesh, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_mesh_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['mesh_stemmed'], row['disease_mesh_count']), axis=1)\n",
    "    trainDataSliced['disease_mesh_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountMesh, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfMesh = tfidfWeights(diseaseTerms, trainDataSliced['mesh_stemmed'])\n",
    "    trainDataSliced['disease_mesh_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfMesh, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    # Gene\n",
    "    geneCountMesh = countTerms(geneTerms, trainDataSliced['mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_mesh_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountMesh, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_mesh_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['mesh_stemmed'], row['gene_mesh_count']), axis=1)\n",
    "    trainDataSliced['gene_mesh_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountMesh, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfMesh = tfidfWeights(geneTerms, trainDataSliced['mesh_stemmed'])\n",
    "    trainDataSliced['gene_mesh_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfMesh, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease + Gene\n",
    "    trainDataSliced['disease_gene_mesh_tf'] = trainDataSliced['disease_mesh_tf'] + trainDataSliced['gene_mesh_tf']\n",
    "    trainDataSliced['disease_gene_mesh_tfidf'] = trainDataSliced['disease_mesh_tfidf'] + trainDataSliced['gene_mesh_tfidf']\n",
    "\n",
    "    # COMBINED\n",
    "\n",
    "    # Disease\n",
    "    termsCountCombined = countTerms(diseaseTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_combined_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountCombined, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_combined_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_abstract_mesh_stemmed'], row['disease_combined_count']), axis=1)\n",
    "    trainDataSliced['disease_combined_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountCombined, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfCombined = tfidfWeights(diseaseTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "    trainDataSliced['disease_combined_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfCombined, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    # Gene\n",
    "    geneCountCombined = countTerms(geneTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_combined_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountCombined, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_combined_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_abstract_mesh_stemmed'], row['gene_combined_count']), axis=1)\n",
    "    trainDataSliced['gene_combined_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountCombined, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfCombined = tfidfWeights(geneTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "    trainDataSliced['gene_combined_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfCombined, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease + Gene\n",
    "    trainDataSliced['disease_gene_combined_tf'] = trainDataSliced['disease_combined_tf'] + trainDataSliced['gene_combined_tf']\n",
    "    trainDataSliced['disease_gene_combined_tfidf'] = trainDataSliced['disease_combined_tfidf'] + trainDataSliced['gene_combined_tfidf']\n",
    "\n",
    "    return trainDataSliced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = trainDataSliced['disease_stemmed'].unique()\n",
    "diseaseTerms = []\n",
    "for disease in diseases:\n",
    "    d = disease.split()\n",
    "    for word in d:\n",
    "        if word not in diseaseTerms:\n",
    "            diseaseTerms.append(word)\n",
    "print(diseaseTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGenes = trainDataSliced['gene_stemmed'].unique()\n",
    "geneTerms = []\n",
    "for genes in allGenes:\n",
    "    d = genes.split()\n",
    "    for gene in d:\n",
    "        if gene not in geneTerms:\n",
    "            geneTerms.append(gene)\n",
    "print(geneTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ari/Downloads/TREC/trec-pm/resources/lexigramOutputTopics2017.json') as f:\n",
    "    exPandedDisease = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = extractFeatures(diseaseTerms, geneTerms, trainDataSliced)\n",
    "pd.options.display.max_columns = None\n",
    "display(trainData.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Testind and *no* Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testValDataSliced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced.append(testValDataSliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Testing and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into validation and testing\n",
    "testDataSliced, valDataSliced, yT, yV = train_test_split(testValDataSliced, testValDataSliced['qid'], test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced.append(testDataSliced)\n",
    "valDataSetSliced.append(valDataSliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDataSetSliced[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Test and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "testDataSetSliced = []\n",
    "valDataSetSliced = []\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=123, shuffle=True)\n",
    "for train_index, test_index in kf.split(testValDataSliced):\n",
    "    testDataSetSliced.append(testValDataSliced.iloc[train_index])\n",
    "    valDataSetSliced.append(testValDataSliced.iloc[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDataSetSliced[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valDataSetSliced[0].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWeights = tvec.fit_transform(trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "trainScore = pd.DataFrame(trainWeights.toarray(), columns=tvec.get_feature_names())\n",
    "trainVoc = tvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainVoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resTrain = pd.concat([trainDataSliced, trainScore, trainDocId], axis=1)\n",
    "trainFinal = resTrain.drop(['title_abstract_mesh_stemmed'], axis=1)\n",
    "trainFinal = trainFinal.sort_values('qid')\n",
    "trainFinal.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankTrain = trainFinal.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"train.txt\", \"w\")\n",
    "\n",
    "for item in rankTrain:\n",
    "    for i,val in item.items():\n",
    "        if(i == \"relevance_score\"):\n",
    "            f.write(str(val)+\" \")\n",
    "        elif(i == \"trec_doc_id\"):\n",
    "            f.write('# '+str(val))\n",
    "        elif(i == \"qid\"):\n",
    "            f.write(str(i)+\":\"+str(val)+\" \")\n",
    "        else:\n",
    "            j = tvec.vocabulary_[i] + 1\n",
    "            f.write(str(j)+\":\"+str(val)+\" \")\n",
    "    f.write(\"\\n\")    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = 0\n",
    "for testDataSliced in testDataSetSliced:\n",
    "    testDocId = testDataSliced[['trec_doc_id']]\n",
    "    testData = testDataSliced[['relevance_score','qid','title_abstract_mesh_stemmed']]\n",
    "    \n",
    "    tvec = TfidfVectorizer(vocabulary = trainVoc)\n",
    "    testWeights = tvec.fit_transform(testData['title_abstract_mesh_stemmed'])\n",
    "    testScore = pd.DataFrame(testWeights.toarray(), columns=tvec.get_feature_names())\n",
    "    \n",
    "    testData.reset_index(drop=True, inplace=True)\n",
    "    testScore.reset_index(drop=True, inplace=True)\n",
    "    testDocId.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    resTest = pd.concat([testData, testScore, testDocId], axis=1)    \n",
    "    testFinal = resTest.drop(['title_abstract_mesh_stemmed'], axis=1)\n",
    "    testFinal = testFinal.sort_values('qid')\n",
    "\n",
    "    rankTest = testFinal.to_dict('records')\n",
    "    \n",
    "    f = open(\"test\"+str(sets)+\".txt\", \"w\")\n",
    "\n",
    "    for item in rankTest:\n",
    "        for i,val in item.items():\n",
    "            if(i == \"relevance_score\"):\n",
    "                f.write(str(val)+\" \")\n",
    "            elif(i == \"trec_doc_id\"):\n",
    "                f.write('# '+str(val))\n",
    "            elif(i == \"qid\"):\n",
    "                f.write(str(i)+\":\"+str(val)+\" \")\n",
    "            else:\n",
    "                j = tvec.vocabulary_[i] + 1\n",
    "                f.write(str(j)+\":\"+str(val)+\" \")\n",
    "        f.write(\"\\n\")    \n",
    "    f.close()\n",
    "    sets = sets + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = 0\n",
    "for valDataSliced in valDataSetSliced:\n",
    "    valDocId = valDataSliced[['trec_doc_id']]\n",
    "    valData = valDataSliced[['relevance_score','qid','title_abstract_mesh_stemmed']]\n",
    "\n",
    "    tvec = TfidfVectorizer(vocabulary = trainVoc)\n",
    "    valWeights = tvec.fit_transform(valData['title_abstract_mesh_stemmed'])\n",
    "    valScore = pd.DataFrame(valWeights.toarray(), columns=tvec.get_feature_names())\n",
    "    \n",
    "    valData.reset_index(drop=True, inplace=True)\n",
    "    valScore.reset_index(drop=True, inplace=True)\n",
    "    valDocId.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    resVal = pd.concat([valData, valScore, valDocId], axis=1)\n",
    "    valFinal = resVal.drop(['title_abstract_mesh_stemmed'], axis=1)\n",
    "    valFinal = valFinal.sort_values('qid')\n",
    "\n",
    "    rankVal = valFinal.to_dict('records')\n",
    "\n",
    "    f = open(\"vali\"+str(sets)+\".txt\", \"w\")\n",
    "\n",
    "    for item in rankVal:\n",
    "        for i,val in item.items():\n",
    "            if(i == \"relevance_score\"):\n",
    "                f.write(str(val)+\" \")\n",
    "            elif(i == \"trec_doc_id\"):\n",
    "                f.write('# '+str(val))\n",
    "            elif(i == \"qid\"):\n",
    "                f.write(str(i)+\":\"+str(val)+\" \")\n",
    "            else:\n",
    "                j = tvec.vocabulary_[i] + 1\n",
    "                f.write(str(j)+\":\"+str(val)+\" \")\n",
    "        f.write(\"\\n\")    \n",
    "    f.close()\n",
    "    sets = sets + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "randonRanking = []\n",
    "ourRanking = []\n",
    "allFeatures = []\n",
    "\n",
    "while count < sets:\n",
    "    metric = pyltr.metrics.NDCG(k=10)\n",
    "\n",
    "    model = pyltr.models.LambdaMART(\n",
    "        metric=metric,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.02,\n",
    "        max_features=0.5,\n",
    "        query_subsample=0.5,\n",
    "        max_leaf_nodes=10,\n",
    "        min_samples_leaf=64,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    print(\"Fold: \"+str(count))\n",
    "    with open('train.txt') as trainfile, open('test'+str(count)+'.txt') as evalfile:\n",
    "        TrainX, Trainy, TrainQids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "        EvalX, Evaly, EvalQids, _ = pyltr.data.letor.read_dataset(evalfile)\n",
    "        \n",
    "    model.fit(TrainX, Trainy, TrainQids)\n",
    "    Epred = model.predict(EvalX)\n",
    "    randonRanking.append(metric.calc_mean_random(EvalQids, Evaly))\n",
    "    ourRanking.append(metric.calc_mean(EvalQids, Evaly, Epred))\n",
    "    \n",
    "    # features\n",
    "    nonZero = np.nonzero(model.feature_importances_)\n",
    "    for i in nonZero:\n",
    "        nonZeros = i.tolist()\n",
    "        \n",
    "    listFeatures = np.argsort(model.feature_importances_)\n",
    "        \n",
    "    for feature in listFeatures:\n",
    "        if (feature in nonZeros) and (feature not in allFeatures):\n",
    "            allFeatures.append(feature)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "randonRanking = []\n",
    "ourRanking = []\n",
    "\n",
    "while count < sets:\n",
    "    metric = pyltr.metrics.NDCG(k=10)\n",
    "\n",
    "    model = pyltr.models.LambdaMART(\n",
    "        metric=metric,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.02,\n",
    "        max_features=0.5,\n",
    "        query_subsample=0.5,\n",
    "        max_leaf_nodes=10,\n",
    "        min_samples_leaf=64,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    print(\"Fold: \"+str(count))\n",
    "    with open('train.txt') as trainfile, open('vali'+str(count)+'.txt') as valifile, open('test'+str(count)+'.txt') as evalfile:\n",
    "        TrainX, Trainy, TrainQids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "        ValX, Valy, ValQids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "        EvalX, Evaly, EvalQids, _ = pyltr.data.letor.read_dataset(evalfile)\n",
    "        \n",
    "    monitor = pyltr.models.monitors.ValidationMonitor(ValX, Valy, ValQids, metric=metric, stop_after=250)\n",
    "    model.fit(TrainX, Trainy, TrainQids, monitor=monitor)\n",
    "    Epred = model.predict(EvalX)\n",
    "    randonRanking.append(metric.calc_mean_random(EvalQids, Evaly))\n",
    "    ourRanking.append(metric.calc_mean(EvalQids, Evaly, Epred))\n",
    "    \n",
    "    # features\n",
    "    nonZero = np.nonzero(model.feature_importances_)\n",
    "    for i in nonZero:\n",
    "        nonZeros = i.tolist()\n",
    "        \n",
    "    listFeatures = np.argsort(model.feature_importances_)\n",
    "        \n",
    "    for feature in listFeatures:\n",
    "        if (feature in nonZeros) and (feature not in allFeatures):\n",
    "            allFeatures.append(feature)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(randonRanking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpR = np.asarray(randonRanking)\n",
    "np.mean(numpR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ourRanking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpR = np.asarray(ourRanking)\n",
    "np.mean(numpR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in allFeatures:\n",
    "    voc = feature-1\n",
    "    for key,value in tvec.vocabulary_.items():\n",
    "        if value == voc:\n",
    "            print(value, key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
